{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d3c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the mat\",\n",
    "    \"the cat chased the dog\",\n",
    "    \"the dog barked at the cat\",\n",
    "    \"the dog barked at the cat\",\n",
    "    \"the cat meowed back at the dog\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04748c5d",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707aea8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
       " ['the', 'dog', 'sat', 'on', 'the', 'mat'],\n",
       " ['the', 'cat', 'chased', 'the', 'dog'],\n",
       " ['the', 'dog', 'barked', 'at', 'the', 'cat'],\n",
       " ['the', 'dog', 'barked', 'at', 'the', 'cat'],\n",
       " ['the', 'cat', 'meowed', 'back', 'at', 'the', 'dog']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [sentence.split() for sentence in corpus]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c386c951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'back', 'barked', 'cat', 'chased', 'dog', 'mat', 'meowed', 'on', 'sat', 'the']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set( word for sentence in tokens for word in sentence))\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i:word for word, i in word_to_ix.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc6f8c",
   "metadata": {},
   "source": [
    "# Generate target context pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a247ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data:  [[10  3]\n",
      " [10  9]\n",
      " [ 3 10]\n",
      " [ 3  9]\n",
      " [ 3  8]]\n"
     ]
    }
   ],
   "source": [
    "def generate_training_data(tokens, window_size=2):\n",
    "    training_data = []\n",
    "    for sentence in tokens:\n",
    "        for i, word in enumerate(sentence):\n",
    "            target = word_to_ix[word]\n",
    "            context_range = range(max(0,i - window_size), min(len(sentence), i + window_size + 1))\n",
    "            for j in context_range:\n",
    "                if i == j:\n",
    "                    continue\n",
    "                training_data.append([target,word_to_ix[sentence[j]]])\n",
    "    return np.array(training_data)\n",
    "\n",
    "\n",
    "training_data = generate_training_data(tokens, window_size=2)\n",
    "print(\"Sample Data: \", training_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76eb27b",
   "metadata": {},
   "source": [
    "# Define a model (2-layer NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fe5813",
   "metadata": {},
   "source": [
    "when we train a word2vec skip gram, we are training here a very small 2 layer neural network\n",
    "\n",
    "input (one hot word) -> W1 -> hidden layer -> W2 -> output (soft max probabilities)\n",
    "\n",
    "input layer: one hot vector of size vocab_size say 5 -> ```[0, 0, 1, 0, 0]```\n",
    "hidden layer: (embedding layer)\n",
    "\n",
    "```\n",
    "hidden = W1.T @ input\n",
    "```\n",
    "\n",
    "this gives 10 dimensional embedding (since embedding_dim is 10)\n",
    "\n",
    "Output layer: \n",
    "```\n",
    "scores = W2.T @ hidden\n",
    "```\n",
    "\n",
    "now the softmax step\n",
    "np.exp(x) means applying the exponential function $e^x$ to each element of `x`\n",
    "- *e* is a mathematicalconstant~ 2.71828\n",
    "- It shows up everywhere in growth, decay, and probabilities.\n",
    "\n",
    "```\n",
    "x = np.array([1, 2, 3])\n",
    "np.exp(x)\n",
    "```\n",
    "\n",
    "gives `[2.718, 7.389, 20.085]`\n",
    "\n",
    "why x - np.max(x) - For numerical stability to avoid huge numbers  that could cause overflow\n",
    "It doesn’t change the relative probabilities, only scales them safely.\n",
    "\n",
    "-- Normalization ---\n",
    "\n",
    "```\n",
    "e_x = np.exp(x - np.max(x))\n",
    "softmax = e_x / e_x.sum(axis=0)\n",
    "\n",
    "```\n",
    "This divides each exponential by the sum of all exponentials, so what happens is:\n",
    "subtraction of max keeps from overflowing \n",
    "\n",
    "we use exponential function e raise to x  because it has special mathematical properties that make it perfect for converting any real numbers into positive smooth and proportionally scaled values that behave well in optimizations. using a different base like 2 raise to x would only scale the ouptuts - it wouldnt chnage the relative probabilities so e^x is chosen for mathematical convenience and smoothness in gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074a303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "W1 = np.random.rand(vocab_size, embedding_dim)\n",
    "W2 = np.random.rand(embedding_dim, vocab_size)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cfd4eb",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a28855",
   "metadata": {},
   "source": [
    "what's inside every training loop?\n",
    "- forward pass - np.dot, np.matmul, activations like softmax, sigmoid - compute predictions\n",
    "- loss calculations - np.sum, np.log etc - compare predictions vs target\n",
    "- backward pass (gradient) - np.dot, np.outer, chain rule - compute how much each weight contributed to error\n",
    "- weight update W -= lr * dW - Adust weights to reduce loss\n",
    "- repeat (epochs) - loop over data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = one-hot (shape = vocab_size × 1)\n",
    "\n",
    "W1 = (vocab_size × embedding_dim)\n",
    "\n",
    "W1.T @ x = (embedding_dim × 1) → gives hidden representation (embedding)\n",
    "\n",
    "W2.T @ h = (vocab_size × 1) → gives scores for all context words\n",
    "\n",
    "softmax turns scores into probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "179ce8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 258.1971\n",
      "Epoch 1000, Loss: 181.4201\n",
      "Epoch 2000, Loss: 181.4935\n",
      "Epoch 3000, Loss: 181.5862\n",
      "Epoch 4000, Loss: 181.6640\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 5000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for target, context in training_data:\n",
    "        # One-hot encode target\n",
    "        x = np.zeros(vocab_size)\n",
    "        x[target] = 1\n",
    "\n",
    "        # Forward pass\n",
    "        h = np.dot(W1.T, x)               # hidden layer (embedding)\n",
    "        u = np.dot(W2.T, h)               # output layer\n",
    "        y_pred = softmax(u)               # predicted prob distribution\n",
    "\n",
    "        # True label\n",
    "        y_true = np.zeros(vocab_size)\n",
    "        y_true[context] = 1\n",
    "\n",
    "        # Loss (cross-entropy)\n",
    "        loss += -np.sum(y_true * np.log(y_pred + 1e-9))\n",
    "\n",
    "        # Backprop\n",
    "        e = y_pred - y_true\n",
    "        dW2 = np.outer(h, e)\n",
    "        dW1 = np.outer(x, np.dot(W2, e))\n",
    "\n",
    "        # Update weights\n",
    "        W1 -= learning_rate * dW1\n",
    "        W2 -= learning_rate * dW2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8c05964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'cat': [ 0.26305422  0.52859225  0.23922563 -0.71310849 -0.1423374  -1.1915748\n",
      "  1.35098558  1.04178593 -0.55792743  0.11334474]\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(word):\n",
    "    return W1[word_to_ix[word]]\n",
    "\n",
    "print(\"Embedding for 'cat':\", get_embedding(\"cat\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b63453ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to 'cat': [('dog', np.float64(0.3713103907085415)), ('back', np.float64(0.36447181396005784)), ('meowed', np.float64(0.3228419668222796))]\n",
      "Most similar to 'dog': [('cat', np.float64(0.3713103907085415)), ('mat', np.float64(0.22315184031014162)), ('barked', np.float64(0.18621481145375948))]\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def most_similar(word, top_n=3):\n",
    "    target_vec = get_embedding(word)\n",
    "    sims = []\n",
    "    for other in vocab:\n",
    "        if other != word:\n",
    "            sim = cosine_similarity(target_vec, get_embedding(other))\n",
    "            sims.append((other, sim))\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    return sims[:top_n]\n",
    "\n",
    "print(\"Most similar to 'cat':\", most_similar(\"cat\"))\n",
    "print(\"Most similar to 'dog':\", most_similar(\"dog\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba25f17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "word2vec-implementation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
